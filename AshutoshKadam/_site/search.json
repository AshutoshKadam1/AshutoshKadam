[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ashutosh Kadam",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! My name is Ashutosh Kadam!"
  },
  {
    "objectID": "Assignment1.html",
    "href": "Assignment1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "data(\"anscombe\") #Load Anscombe Data\nView(anscombe) #View the data\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n\n\n\n\n\nplot(anscombe$x1,anscombe$y1)\n\n\n\n\n\n\n\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n\n\n\n\n\n\nlm1 &lt;- lm(y1 ~ x1, data = anscombe)\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\nlm2 &lt;- lm(y2 ~ x2, data = anscombe)\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\nlm3 &lt;- lm(y3 ~ x3, data = anscombe)\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\nlm4 &lt;- lm(y4 ~ x4, data = anscombe)\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\nplot(anscombe$x1,anscombe$y1)\nabline(coefficients(lm1))\n\n\n\n\n\n\n\nplot(anscombe$x2,anscombe$y2)\nabline(coefficients(lm2))\n\n\n\n\n\n\n\nplot(anscombe$x3,anscombe$y3)\nabline(coefficients(lm3))\n\n\n\n\n\n\n\nplot(anscombe$x4,anscombe$y4)\nabline(coefficients(lm4))\n\n\n\n\n\n\n\n\n\n\n\nff &lt;- y ~ x\nmods &lt;- setNames(as.list(1:4), paste0(\"lm\", 1:4))\n\n\n\n\n\n\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  ## or   ff[[2]] &lt;- as.name(paste0(\"y\", i))\n  ##      ff[[3]] &lt;- as.name(paste0(\"x\", i))\n  mods[[i]] &lt;- lmi &lt;- lm(ff, data = anscombe)\n  print(anova(lmi))\n}\n\nAnalysis of Variance Table\n\nResponse: y1\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nx1         1 27.510 27.5100   17.99 0.00217 **\nResiduals  9 13.763  1.5292                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y2\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx2         1 27.500 27.5000  17.966 0.002179 **\nResiduals  9 13.776  1.5307                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y3\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx3         1 27.470 27.4700  17.972 0.002176 **\nResiduals  9 13.756  1.5285                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y4\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx4         1 27.490 27.4900  18.003 0.002165 **\nResiduals  9 13.742  1.5269                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsapply(mods, coef)  # Note the use of this function\n\n                  lm1      lm2       lm3       lm4\n(Intercept) 3.0000909 3.000909 3.0024545 3.0017273\nx1          0.5000909 0.500000 0.4997273 0.4999091\n\nlapply(mods, function(fm) coef(summary(fm)))\n\n$lm1\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0000909  1.1247468 2.667348 0.025734051\nx1          0.5000909  0.1179055 4.241455 0.002169629\n\n$lm2\n            Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.000909  1.1253024 2.666758 0.025758941\nx2          0.500000  0.1179637 4.238590 0.002178816\n\n$lm3\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0024545  1.1244812 2.670080 0.025619109\nx3          0.4997273  0.1178777 4.239372 0.002176305\n\n$lm4\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0017273  1.1239211 2.670763 0.025590425\nx4          0.4999091  0.1178189 4.243028 0.002164602\n\n\n\n\n\n\nop &lt;- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 2, 0))\n\n\n\n\n\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  plot(ff, data = anscombe, col = \"red\", pch = 21, bg = \"orange\", cex = 1.2,\n       xlim = c(3, 19), ylim = c(3, 13))\n  abline(mods[[i]], col = \"blue\")\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmtext(\"Anscombe's 4 Regression data sets\", outer = TRUE, cex = 1.5)\n\n\n\n\n\n\n\npar(op)"
  },
  {
    "objectID": "Assignment1.html#data-visualization",
    "href": "Assignment1.html#data-visualization",
    "title": "Assignment 1",
    "section": "Data Visualization",
    "text": "Data Visualization"
  },
  {
    "objectID": "Assignment1.html#objective-identify-data-or-model-problems-using-visualization",
    "href": "Assignment1.html#objective-identify-data-or-model-problems-using-visualization",
    "title": "Assignment 1",
    "section": "Objective: Identify data or model problems using visualization",
    "text": "Objective: Identify data or model problems using visualization"
  },
  {
    "objectID": "Assignment1.html#anscombe-1973-quartlet",
    "href": "Assignment1.html#anscombe-1973-quartlet",
    "title": "Assignment 1",
    "section": "",
    "text": "data(\"anscombe\") #Load Anscombe Data\nView(anscombe) #View the data\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500"
  },
  {
    "objectID": "Assignment1.html#simple-version",
    "href": "Assignment1.html#simple-version",
    "title": "Assignment 1",
    "section": "",
    "text": "plot(anscombe$x1,anscombe$y1)\n\n\n\n\n\n\n\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500"
  },
  {
    "objectID": "Assignment1.html#fancy-version-per-file-help",
    "href": "Assignment1.html#fancy-version-per-file-help",
    "title": "Assignment 1",
    "section": "",
    "text": "ff &lt;- y ~ x\nmods &lt;- setNames(as.list(1:4), paste0(\"lm\", 1:4))"
  },
  {
    "objectID": "Assignment1.html#generative-art-examples",
    "href": "Assignment1.html#generative-art-examples",
    "title": "Assignment 1",
    "section": "Generative Art Examples:",
    "text": "Generative Art Examples:\n!Schotter(Gravel) - George Nees, 1968\n!Path - Casey Reas, 2001\n!VVRRR - Manolo April, 2018"
  },
  {
    "objectID": "Assignment1.html#data-visualization-1",
    "href": "Assignment1.html#data-visualization-1",
    "title": "Assignment 1",
    "section": "Data Visualization",
    "text": "Data Visualization"
  },
  {
    "objectID": "Assignment1.html#objective-create-graphics-with-r",
    "href": "Assignment1.html#objective-create-graphics-with-r",
    "title": "Assignment 1",
    "section": "Objective: Create graphics with R",
    "text": "Objective: Create graphics with R"
  },
  {
    "objectID": "Assignment1.html#title-fall-color",
    "href": "Assignment1.html#title-fall-color",
    "title": "Assignment 1",
    "section": "Title: Fall color",
    "text": "Title: Fall color"
  },
  {
    "objectID": "Assignment1.html#critique-of-a-chart-in-earth-observatorys-article-on-global-temperatures",
    "href": "Assignment1.html#critique-of-a-chart-in-earth-observatorys-article-on-global-temperatures",
    "title": "Assignment 1",
    "section": "Critique of a Chart in Earth Observatory’s Article on Global Temperatures:",
    "text": "Critique of a Chart in Earth Observatory’s Article on Global Temperatures:\nContext:\nThis chart attempts to show the global temperature trends in the past 100+ years. The purpose is to note the trend of rising temperatures, especially in the last decade, and the relationship between specific industrial and economic milestones with climate change.\nClarity:\nDue to the large amount of data in present in the x axis(years), it can be heard to pinpoint the global temperature anomaly per year and even note the changes between the year. As you go towards to present, it gets harder to keep track of the temperature changes. However, with the color contrasts such as deeper shades of red, it does well to compensate.\nAccuracy:\nOnce again while the accuracy is most likely high given the research and sources provided in the article itself, the lack of labeling make it hard to follow, especially with pinpointing the exact years of the notable historical events such as World War II.\nDesign Choices:\nWhile the simplicity of this chart is commendable and for the most part its easy to read, I believe that multiple charts would allow for information to be conveyed more easily to the readily. Especially since the chart is titled “Last 9 Years Warmest on Record” but is the hardest to read with the the y-axis being on the left. Apart from multiple charts, the easiest way to fix that would be to move the y-axis to the right or adding a legend that allows you to compare the shades of colors to temperature changes.\nNarrative Support:\nI believe that thanks to large variety of data provided and a clear trend in the change in temperature, the chart does an excellent job at highlighting the drastic change in temperature over the this century especially. Additionally, with the explanation in the article about the history of mankind associated with climate change the chart does well to support the narrative, despite being somewhat hard to read towards the more recent years."
  },
  {
    "objectID": "PrepareForClass.html",
    "href": "PrepareForClass.html",
    "title": "Notes and Reviews",
    "section": "",
    "text": "The Microsoft Machine Learning & Data Science Summit 2016 hosted a captivating keynote session by Dr. Edward Tufte, a notable figure in the field data visualization and analytics. Known for his work on presenting data in intuitive and insightful ways, Tufte’s talk, titled The Future, focused on the evolving landscape of data science and its intersection with effective communication and design.\n\n\n\nThe Art of Data Visualization: Tufte emphasized the power of visualization in making sense of complex data. He argued that the future of data science heavily relies on how well data is communicated visually. Tufte reinforced his belief that effective visualizations not only present data but also evoke deeper understanding and inspire action. His approach bridges the technical and creative aspects of data science, urging professionals to be as much designers as they are scientists.\nData-Driven Storytelling: A significant theme was the concept of data-driven storytelling. Tufte stressed that the future belongs to those who can tell compelling stories through data. He showcased examples of how well-crafted narratives using data can be much more impactful than mere statistical analyses. Tufte’s approach to storytelling is grounded in the idea that human beings are inherently narrative-driven, and good data science must cater to that by making data relatable and accessible.\nHuman-Centered Design in Machine Learning: Tufte also touched upon machine learning’s increasing role in the world of data science but was clear that human-centered design must not be lost in the excitement of automation and AI. He pointed out that machine learning models are only as good as their ability to provide actionable insights to people. He urged machine learning practitioners to stay mindful of how their models and algorithms will interact with human users, advocating for transparency and interpretability in AI systems.\nSimplicity and Clarity in Data: True to his reputation, Tufte pushed for simplicity and clarity in data visualization. He criticized overly complex charts and unnecessary embellishments, which often obscure the insights buried in data. Tufte called for clear, precise, and minimalist designs that allow the data to speak for itself, a mantra that remains highly relevant in the era of big data and machine learning, where clarity is often sacrificed for complexity.\nThe Role of Ethics: While not a central focus, Tufte briefly acknowledged the ethical implications of data science and machine learning. He discussed the importance of being responsible with how data is represented and ensuring that the stories told through data are honest and grounded in fact. This aspect of his talk is particularly relevant in today’s data-driven decision-making environments, where biases and misrepresentations can have far-reaching consequences.\n\n\n\n\nTufte’s keynote resonated with attendees and and me as a viewer. His call for thoughtful design and careful consideration of how data is presented remains incredibly pertinent. As machine learning continues to advance, Tufte’s insights serve as a timely reminder that while technology evolves, the need for clear, impactful communication remains constant.\nIn summary, Dr. Edward Tufte’s keynote was a compelling blend of visionary ideas and practical guidance. His message underscored that the future of machine learning and data science will be driven not just by algorithms and computation but by how well data scientists can communicate their insights in a way that is both meaningful and actionable. Tufte’s insistence on the importance of human-centered design and storytelling provides a valuable perspective for the next generation of data practitioners."
  },
  {
    "objectID": "PrepareForClass.html#a-review-of-the-future-by-dr.-edward-tufte",
    "href": "PrepareForClass.html#a-review-of-the-future-by-dr.-edward-tufte",
    "title": "Notes and Reviews",
    "section": "",
    "text": "The Microsoft Machine Learning & Data Science Summit 2016 hosted a captivating keynote session by Dr. Edward Tufte, a notable figure in the field data visualization and analytics. Known for his work on presenting data in intuitive and insightful ways, Tufte’s talk, titled The Future, focused on the evolving landscape of data science and its intersection with effective communication and design.\n\n\n\nThe Art of Data Visualization: Tufte emphasized the power of visualization in making sense of complex data. He argued that the future of data science heavily relies on how well data is communicated visually. Tufte reinforced his belief that effective visualizations not only present data but also evoke deeper understanding and inspire action. His approach bridges the technical and creative aspects of data science, urging professionals to be as much designers as they are scientists.\nData-Driven Storytelling: A significant theme was the concept of data-driven storytelling. Tufte stressed that the future belongs to those who can tell compelling stories through data. He showcased examples of how well-crafted narratives using data can be much more impactful than mere statistical analyses. Tufte’s approach to storytelling is grounded in the idea that human beings are inherently narrative-driven, and good data science must cater to that by making data relatable and accessible.\nHuman-Centered Design in Machine Learning: Tufte also touched upon machine learning’s increasing role in the world of data science but was clear that human-centered design must not be lost in the excitement of automation and AI. He pointed out that machine learning models are only as good as their ability to provide actionable insights to people. He urged machine learning practitioners to stay mindful of how their models and algorithms will interact with human users, advocating for transparency and interpretability in AI systems.\nSimplicity and Clarity in Data: True to his reputation, Tufte pushed for simplicity and clarity in data visualization. He criticized overly complex charts and unnecessary embellishments, which often obscure the insights buried in data. Tufte called for clear, precise, and minimalist designs that allow the data to speak for itself, a mantra that remains highly relevant in the era of big data and machine learning, where clarity is often sacrificed for complexity.\nThe Role of Ethics: While not a central focus, Tufte briefly acknowledged the ethical implications of data science and machine learning. He discussed the importance of being responsible with how data is represented and ensuring that the stories told through data are honest and grounded in fact. This aspect of his talk is particularly relevant in today’s data-driven decision-making environments, where biases and misrepresentations can have far-reaching consequences.\n\n\n\n\nTufte’s keynote resonated with attendees and and me as a viewer. His call for thoughtful design and careful consideration of how data is presented remains incredibly pertinent. As machine learning continues to advance, Tufte’s insights serve as a timely reminder that while technology evolves, the need for clear, impactful communication remains constant.\nIn summary, Dr. Edward Tufte’s keynote was a compelling blend of visionary ideas and practical guidance. His message underscored that the future of machine learning and data science will be driven not just by algorithms and computation but by how well data scientists can communicate their insights in a way that is both meaningful and actionable. Tufte’s insistence on the importance of human-centered design and storytelling provides a valuable perspective for the next generation of data practitioners."
  },
  {
    "objectID": "PrepareForClass.html#a-note-on-big-data-analytics-pitfalls",
    "href": "PrepareForClass.html#a-note-on-big-data-analytics-pitfalls",
    "title": "Notes and Reviews",
    "section": "A Note on Big Data Analytics Pitfalls",
    "text": "A Note on Big Data Analytics Pitfalls\nWhile Big Data analytics holds significant promise for valuable insights, there are some problems that data scientists may encounter. The first and most obvious one is data quality. Aside from inconsistencies and inaccuracies in data that could lead to misleading conclusions, the process of data collection in itself can be be a time consuming and resource intensive process that is difficult to maintain. In addition to the process of collecting data, another pitfall could be the handling and processing of vast amounts of data. This process itself also requires significant computational resources, which without innovative solutions and infrastructure, organizations and data scientists may struggle with performance issues. Additionally, even when analyzing and interpreting data comes the age old problem of misinterpreting correlation as causation. This can lead to incorrect scientific and business conclusions which can cause catastrophic consequences. Finally, on the ethical side of things with data analytics is the issue of privacy and security. With larger data sets, especially those containing personal or sensitive information, data privacy and security is critical. Failing to secure data properly or violating ethical and legal guidelines to acquire such data can lead to severe legal, ethical and scientific ramifications that can set the field of data science back decades."
  },
  {
    "objectID": "PrepareForClass.html#a-note-on-overfitting-and-overparameterization",
    "href": "PrepareForClass.html#a-note-on-overfitting-and-overparameterization",
    "title": "Notes and Reviews",
    "section": "A Note on Overfitting and Overparameterization",
    "text": "A Note on Overfitting and Overparameterization\nOverfitting is a phenomena that occurs when a machine learning model becomes too complex and learns not only the underlying patterns in the learning data but also the noise or irrelevant details. This results in a model that performs very well on the training data but fails to generalize to new, unseen data. Overfitting is more likely when the model has too may parameters relative to the size of the data set, or the model is too complex and if there is a lack of regularization techniques to control model complexity. An outcome of overfitting includes high accuracy on the training set by poor performance on validation or test sets. Another outcome is erratic and inaccurate predictions of future/newer data sets.\nOverparameterization refers to the situation where a model has more parameters than necessary to fit the training data. As mentioned above, this often leads to overfitting, as the model can become overly flexible and capture noise or outliers. However, recent research in deep learning has shown the overparameterized models can sometimes still generalize well, especially when trained with techniques like early stopping, regularization or dropout. Similar to overfitting, some causes are excessive model complexity, insufficient training data and lack of regularization. In order to prevent overparameterization, using techniques such as cross validation, regularization, pruning/early stopping and a data augmentation can be greatly effective."
  },
  {
    "objectID": "PrepareForClass.html#a-summary-of-hadley-wikhams-embl-keynote",
    "href": "PrepareForClass.html#a-summary-of-hadley-wikhams-embl-keynote",
    "title": "Notes and Reviews",
    "section": "A Summary of Hadley Wikham’s EMBL Keynote",
    "text": "A Summary of Hadley Wikham’s EMBL Keynote\nIn Hadley Wickham’s EMBL keynote on data visualization and data science, he introduces several key technologies and techniques related to the R programming language, particularly focusing on the “tidyverse” ecosystem. Tidyverse is a collection of R packages designed to streamline and enhance data science workflows, making them more intuitive and accessible. Some of the prominent tools within tidyverse that Wickham discusses include:\n\nggplot2: A powerful data visualization package that makes it easier to create complex, multi-layered plots.\ndplyr: For data manipulation, offering simple and efficient ways to filter, summarize, and transform data.\ntidyr: Helps in tidying data, transforming datasets into formats suitable for analysis.\npurrr: Focuses on functional programming paradigms to work with lists and data structures effectively.\nreadr: Simplifies the data import process.\n\nWickham’s main points center around making data science tools user-friendly and accessible, particularly through the tidyverse. He emphasizes the importance of code being readable, reproducible and shareable, which fosters collaboration and better communication among data scientists. His analogy of coding as a language—one that needs to be deconstructed and learned for fluency—highlights his philosophy of making programming inclusive and accessible to a broader audience, not just experienced programmers.\nMoreover, Wickham touches on the concept of orthogonal components in data, emphasizing that separating data features logically leads to more effective visualizations and analysis. His demonstration using the Gapminder dataset effectively showcases the application of tidyverse tools to real-world data science problems. Wickham also stresses the human side of data science, arguing that good design in this field isn’t solely about writing efficient code but also about fostering empathy, practicality, and iterative improvement. He notes that these principles help build tools that lead users to success by making good practices easy to adopt, rather than challenging to achieve. In essence, Wickham advocates for a blend of technical rigor and user-centered design in data science, aiming to democratize the field and make it more approachable for everyone​."
  },
  {
    "objectID": "Assignment2.html#start-plotting-from-basics",
    "href": "Assignment2.html#start-plotting-from-basics",
    "title": "Assignment 2",
    "section": "Start plotting from basics",
    "text": "Start plotting from basics\n\nplot(pressure, pch=16)  # Can you change pch?\ntext(150, 600, \n     \"Pressure (mm Hg)\\nversus\\nTemperature (Celsius)\")\n\n\n\n\n\n\n\n\nQuestion: Can you change the pch?\nYes, you can change it. It will change the shapes and fill the plots in the graph."
  },
  {
    "objectID": "Assignment2.html#using-plotting-functions-to-recreate-above-graphs-using-happy-planet-data",
    "href": "Assignment2.html#using-plotting-functions-to-recreate-above-graphs-using-happy-planet-data",
    "title": "Assignment 2",
    "section": "Using plotting functions to recreate above graphs using Happy Planet Data",
    "text": "Using plotting functions to recreate above graphs using Happy Planet Data\n\nlibrary(ggplot2)\nlibrary(readxl)\nhpd &lt;- read_excel(\"C:/Users/Ashu/Downloads/HPI_2024_public_dataset.xlsx\", \n    sheet = \"1. All countries\", range = \"A9:L158\")\n\nNew names:\n• `` -&gt; `...4`\n\nView(hpd)"
  },
  {
    "objectID": "Assignment4.html",
    "href": "Assignment4.html",
    "title": "Ashutosh Kadam",
    "section": "",
    "text": "library(ggplot2)\ndata(diamonds)\nggplot(diamonds, aes(cut)) +\n  geom_bar(fill = \"skyblue\", color = \"black\") +\n  geom_text(stat = 'count',aes(label = after_stat(count)), vjust = -0.5, size = 3) +\n  labs(title = \"Bar Chart of Diamond Cuts\",\n       x = \"Cut\",\n       y = \"Count\") +\n  theme_minimal()"
  }
]